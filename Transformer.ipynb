{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from the local file\n",
    "data = np.load('E:\\\\Transformer Data\\\\AirRaid-v0_10x1000_0.npy', allow_pickle =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state array\n",
    "state_arr = data[0][0]['state']\n",
    "\n",
    "# number input feature to Encoder\n",
    "N = 256\n",
    "# number of feature sets\n",
    "n_x = state_arr.shape[1]*state_arr.shape[2]*state_arr.shape[0]\n",
    "# the steps values will be replaced from the real data..length of trajectory\n",
    "# steps = data.shape[1]\n",
    "\n",
    "\n",
    "''''''\n",
    "steps = 512\n",
    "N = 256\n",
    "''''''\n",
    "\n",
    "\n",
    "\n",
    "# # number of pixel per state\n",
    "# N = 255\n",
    "# # the steps values will be replaced from the real data..length of trajectory\n",
    "# steps = 1000 \n",
    "\n",
    "# number of query\n",
    "dk = 64\n",
    "# number of value\n",
    "dv = 64\n",
    "\n",
    "# number of head\n",
    "n_h = 8\n",
    "\n",
    "# dropout rate\n",
    "Pdrop = 0.1 # this is same as the attention research paper\n",
    "\n",
    "# FFN weight dimensionality\n",
    "dff = 2048 # this number is computationally expensive\n",
    "#dff = 64\n",
    "\n",
    "\n",
    "# number of epoch\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "# m = 1 \n",
    "# input dummy states for 1000 steps\n",
    "# S = np.random.rand(steps, N)\n",
    "# # actions for each state\n",
    "# A = np.zeros((steps,1), dtype=int)\n",
    "\n",
    "# initial input state\n",
    "init_S = np.empty((steps, n_x))\n",
    "\n",
    "# input empty states to the encoder for 1000 steps\n",
    "S = np.empty((steps, N))\n",
    "\n",
    "# empty actions for each state\n",
    "A = np.empty((steps,1), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_action(data, empty_state, empty_action, steps, n_x):\n",
    "    '''\n",
    "    parameters:\n",
    "        data: games data \n",
    "        empty_state: empty array of state\n",
    "        empty_action: empty array of action\n",
    "        steps: Number of steps in the trajectory\n",
    "        n_x: number of feature set\n",
    "    return:\n",
    "        init_S: input states of N steps\n",
    "        A: actions of N steps\n",
    "    '''\n",
    "    \n",
    "    init_S = empty_state\n",
    "    A = empty_action\n",
    "\n",
    "    for i in range(0,steps):\n",
    "        X = data[0][i]['state']\n",
    "        init_S[i, :] = X.reshape(n_x,1).T\n",
    "        \n",
    "        # create some randomness in the data\n",
    "#         init_S[i, :] = init_S[i, :] + np.random.randint(10,250, n_x) #'''I will remove this when data is valid'''\n",
    "\n",
    "        init_S[i, :] = init_S[i, :]\n",
    "        \n",
    "        A[i, :] = data[0][i]['action']\n",
    "        \n",
    "    return(init_S, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will reduce dimentionality of feature set\n",
    "def reduce_dim(init_S, N):\n",
    "    '''\n",
    "    parameters:\n",
    "        init_S: initial input state\n",
    "        N: number input feature to Encoder\n",
    "    return:\n",
    "        S: input state of encoder\n",
    "    '''\n",
    "    pca = PCA(n_components=N)\n",
    "    S = pca.fit_transform(init_S)\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_S, A = get_state_action(data, init_S, A, steps, n_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = reduce_dim(init_S, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X = S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(A)\n",
    "# A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #assign first 500 indices to -1 and rest to 1\n",
    "# A[0:int(steps/3),] = 0\n",
    "# A[int(steps/3):int(steps/3)*2,] = 1\n",
    "# A[int(steps/3)*2:steps,] = 2\n",
    "\n",
    "\n",
    "# # A[0:499,] = 0\n",
    "# A[499:1000,] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_c = len(np.unique(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function intializes weights used in encoder, decoder(masked) for query, value and key\n",
    "def initialize_weights(n_h, N, dk, dv):\n",
    "    '''\n",
    "    parameters:\n",
    "        n_h: number of heads \n",
    "        N: number of pixel per state\n",
    "        dk: number of query\n",
    "        dv: number of value\n",
    "    return:\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    ## ENCODER WEIGTHS ##\n",
    "    lst_encoder_weights = []\n",
    "    for i in range(0,n_h):\n",
    "        encoder_weights = {}\n",
    "        WiQ = 'W' + str(i) + 'Q'\n",
    "        WiK = 'W' + str(i) + 'K'\n",
    "        WiV = 'W' + str(i) + 'V'\n",
    "        \n",
    "        encoder_weights[WiQ] = tf.get_variable(WiQ, [dk, N], \n",
    "                                                     initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "        encoder_weights[WiK] = tf.get_variable(WiK, [dk, N], \n",
    "                                                     initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "        encoder_weights[WiV] = tf.get_variable(WiV, [dv, N],\n",
    "                                                     initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "        lst_encoder_weights.append(encoder_weights)\n",
    "    \n",
    "    ## MASKED DECODER WEIGTHS ##\n",
    "    lst_masked_decoder_weights = []\n",
    "    for i in range(0,n_h):\n",
    "        masked_decoder_weights = {}\n",
    "        md_WiQ = 'md_W' + str(i) + 'Q'\n",
    "        md_WiK = 'md_W' + str(i) + 'K'\n",
    "        md_WiV = 'md_W' + str(i) + 'V'\n",
    "        \n",
    "        masked_decoder_weights[md_WiQ] = tf.get_variable(md_WiQ, [dk, 1], \n",
    "                                                     initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "        masked_decoder_weights[md_WiK] = tf.get_variable(md_WiK, [dk, 1], \n",
    "                                                     initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "        masked_decoder_weights[md_WiV] = tf.get_variable(md_WiV, [dv, 1],\n",
    "                                                     initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "        lst_masked_decoder_weights.append(masked_decoder_weights)\n",
    "\n",
    "    ## DECODER WEIGTHS ##\n",
    "    lst_decoder_weights = []\n",
    "    for i in range(0,n_h):\n",
    "        decoder_weights = {}\n",
    "        d_WiQ = 'd_W' + str(i) + 'Q'\n",
    "        d_WiK = 'd_W' + str(i) + 'K'\n",
    "        d_WiV = 'd_W' + str(i) + 'V'\n",
    "        \n",
    "        decoder_weights[d_WiQ] = tf.get_variable(d_WiQ, [dk, N], \n",
    "                                                     initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "        decoder_weights[d_WiK] = tf.get_variable(d_WiK, [dk, N], \n",
    "                                                     initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "        decoder_weights[d_WiV] = tf.get_variable(d_WiV, [dv, N],\n",
    "                                                     initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "        lst_decoder_weights.append(decoder_weights)\n",
    "    \n",
    "    return (lst_encoder_weights, lst_masked_decoder_weights, lst_decoder_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will initialize multi head attention weights\n",
    "def initialize_multihead_attention_weights(N, dk, n_h):\n",
    "    '''\n",
    "    parameters:\n",
    "    N: number of pixel per state\n",
    "    dk: number of query\n",
    "    n_h: number of heads\n",
    "    '''\n",
    "    ## try to reduce dimension by dividin \n",
    "    ## N by 8\n",
    "    ###\n",
    "    \n",
    "    multi_head_attention_weight = tf.get_variable('W1O', [dk*n_h, N], \n",
    "                                                     initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "\n",
    "    masked_multi_head_attention_weight = tf.get_variable('md_W1O', [dk*n_h, N], \n",
    "                                                     initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    \n",
    "    decoder_multi_head_attention_weight = tf.get_variable('d_W1O', [dk*n_h, N], \n",
    "                                                     initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "\n",
    "    return (multi_head_attention_weight, masked_multi_head_attention_weight, decoder_multi_head_attention_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will initialize weights for the feed forward layer\n",
    "def initialize_FFN_weights(N, dff):\n",
    "    '''\n",
    "    parameters:\n",
    "        N: number of pixel per state\n",
    "        dff: feed foward network dimensionality\n",
    "    return:\n",
    "        FFN_weight: Feed forwad network weight\n",
    "    '''\n",
    "    FFN_weights = {}\n",
    "    \n",
    "    FFN_weights['ffnW1'] = tf.get_variable('ffnW1', [N, dff],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    FFN_weights['ffnW2'] = tf.get_variable('ffnW2', [dff, N],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "\n",
    "    ## ffw for decoder stack\n",
    "    D_FFN_weights = {}\n",
    "    \n",
    "    D_FFN_weights['d_ffnW1'] = tf.get_variable('d_ffnW1', [N, dff],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    D_FFN_weights['d_ffnW2'] = tf.get_variable('d_ffnW2', [dff, N],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    \n",
    "    \n",
    "    return (FFN_weights, D_FFN_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis function will initialize bias for Feed forward lawyer\n",
    "def initialize_bias(N, dff):\n",
    "    '''\n",
    "    parameters:\n",
    "        N: number of pixel per state\n",
    "        dff: feed foward network dimensionality\n",
    "    return:\n",
    "        b: bias\n",
    "    '''\n",
    "    b_weights = {}\n",
    "    \n",
    "    b_weights['b1'] = tf.get_variable('b1', [dff],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "\n",
    "    b_weights['b2'] = tf.get_variable('b2', [N],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    \n",
    "    # biases for decoder stack\n",
    "    d_b_weights = {}\n",
    "    \n",
    "    d_b_weights['d_b1'] = tf.get_variable('d_b1', [dff],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "\n",
    "    d_b_weights['d_b2'] = tf.get_variable('d_b2', [N],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer(seed=1))    \n",
    "\n",
    "    return (b_weights, d_b_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will initialize parameter for output layer\n",
    "def initialize_weight_output(N, n_c):\n",
    "    '''\n",
    "    parameters:\n",
    "        N: number of features\n",
    "    return:\n",
    "        W_out: weights for output layer\n",
    "        b_out: bias for output layer\n",
    "    '''\n",
    "    \n",
    "    # in case we have multi class problem then deminsion will be changed from [N,1]\n",
    "    # use [N,n_c] where n_c is number of classes\n",
    "    W_out = tf.get_variable('W_out', [N,n_c],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    \n",
    "    b_out = tf.get_variable('b_out', [1],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    return W_out, b_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W, md_W, d_W = initialize_weights(n_h, N, dk, dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# invoke all weights & biases here as global variables\n",
    "\n",
    "W, md_W, d_W = initialize_weights(n_h, N, dk, dv)\n",
    "mha_W, m_mha_W, d_mha_W = initialize_multihead_attention_weights(N, dk, n_h)\n",
    "ffn_W, d_ffn_W = initialize_FFN_weights(N, dff)\n",
    "\n",
    "b_weights, d_b_weights = initialize_bias(N, dff)\n",
    "\n",
    "W_out, b_out = initialize_weight_output(N, n_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Note: Positional Encoding)** Need to resolve assignment of positional encoding because when value is too small it does effect on the input state wehreas when value is increased then it effect the output value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis function calculates positional encoding of the input\n",
    "def positional_encoding(input_data, N, steps):\n",
    "    '''\n",
    "    parameters:\n",
    "        input_data: input state\n",
    "        N: number of pixel per state\n",
    "        steps: length of trajectory\n",
    "    return:\n",
    "        new states with positional embedings\n",
    "    '''\n",
    "    # pos is postions of state in the dimension and is the vector \n",
    "    pos = np.array([i+1 for i in range(0, input_data.shape[0])])\n",
    "    \n",
    "    # i is the dimension and will be equal to steps\n",
    "    i = steps\n",
    "    \n",
    "    # d_model is same as N(number of pixels per state)\n",
    "    d_model= N\n",
    "    \n",
    "    #calculcate positional embeding with sin function\n",
    "    PE = np.sin(pos/(10000**(2*i/d_model)))\n",
    "#     PE = np.sin(pos/(10**(2*i/d_model))) # since 10000 has negligible effect it is reduced from 10000 to 10\n",
    "    \n",
    "    # add positional encoding to each of state of trajectory\n",
    "    for i in range(0, PE.shape[0]):\n",
    "        input_data[i,:] = input_data[i,:] + PE[i,]\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is temporary function to convert sample input numpy array to tensorflow data\n",
    "def convert_numpyPE_to_tfPE(S, N, steps):\n",
    "    '''\n",
    "    parameters:\n",
    "        S: input state\n",
    "        N: number of pixel per state\n",
    "        steps: length of trajectory\n",
    "    return:\n",
    "        data_tf: converted PE in tf\n",
    "    '''\n",
    "    data_tf = tf.convert_to_tensor(positional_encoding(S, N, steps), np.float32)\n",
    "\n",
    "#     int_sess = tf.InteractiveSession()  \n",
    "##     print(data_tf.eval())\n",
    "\n",
    "#     int_sess.close()\n",
    "    \n",
    "    return data_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_output(Z, time_stamp = 0, mask_value = -1e9):\n",
    "    \n",
    "    \n",
    "    mask_data = tf.fill(Z.shape, mask_value)\n",
    "\n",
    "    np_mask_idx = np.zeros(Z.shape, dtype= bool)\n",
    "\n",
    "    for i in range(0, time_stamp):\n",
    "        np_mask_idx[i,:] = True\n",
    "        \n",
    "    mask_idx = tf.convert_to_tensor(np_mask_idx)\n",
    "\n",
    "    mask_Z = tf.where(mask_idx, Z, mask_data)\n",
    "    \n",
    "    return mask_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will caluculate Q, K and V\n",
    "def get_q_k_v(query_data, key_data, value_data, WQ, WK, WV):\n",
    "    '''\n",
    "    parameters:\n",
    "        query_data: data to calculate query\n",
    "        key_data: data to calculate key\n",
    "        value_data: data to calculate value\n",
    "        WQ: query weights\n",
    "        WK: key weights\n",
    "        WV: value weights\n",
    "    return:\n",
    "        Q: query\n",
    "        K: key\n",
    "        V: value\n",
    "    '''\n",
    "    \n",
    "    Q = tf.matmul(query_data,tf.transpose(WQ))\n",
    "    K = tf.matmul(key_data,tf.transpose(WK))\n",
    "    V = tf.matmul(value_data,tf.transpose(WV))\n",
    "    \n",
    "    return (Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the attention function will calculate the z value for given weights, and positional encoded inputs\n",
    "# def calaculate_single_attention(S, WQ, WK, WV, dk, mask = False):\n",
    "def calaculate_single_attention(Q, K, V, dk, mask = False, time_stamp = 0):\n",
    "    '''\n",
    "    parameters:\n",
    "        Q: query \n",
    "        K: key  \n",
    "        N: value  \n",
    "        dk: number of query\n",
    "        mask: implement mask on illegal connection\n",
    "    return:\n",
    "        Z: attention head\n",
    "    '''\n",
    "    \n",
    "#     # calculate query, value and key\n",
    "#     Q = tf.matmul(S,tf.transpose(WQ))\n",
    "#     K = tf.matmul(S,tf.transpose(WK))\n",
    "#     V = tf.matmul(S,tf.transpose(WV))\n",
    "    \n",
    "    # linear activation of query and key\n",
    "    matmul_z1 = tf.matmul(Q,tf.transpose(K)) # in my computational grapy matmul_z1 is A\n",
    "    \n",
    "    # to mask values of the output call masking function to get masked value (-inf)\n",
    "    if(mask == True):\n",
    "        matmul_z1 = mask_output(matmul_z1, time_stamp)\n",
    "#         return matmul_z1\n",
    "    \n",
    "    # non-linear activation \n",
    "    softmax_a = tf.nn.softmax(tf.divide(matmul_z1,np.sqrt(dk))) # in my computational grapy softmax_a is B\n",
    "    \n",
    "    # head of the attention\n",
    "    Z = tf.matmul(softmax_a, V) # attention head\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function will concate n number of head to single head\n",
    "def multi_head_attention(query_data, key_data, value_data, dk, n_h, W, mha_W, mask = False):\n",
    "    '''\n",
    "    parameters:\n",
    "        query_data: data to calculate query\n",
    "        key_data: data to calculate key\n",
    "        value_data: data to calculate value\n",
    "        dk: number of query\n",
    "        n_h: number of heads\n",
    "        W: Input encoder weight  ## for now only\n",
    "        mha_W: multihead_attention_weights\n",
    "    return:\n",
    "        Z_: concated single head\n",
    "    '''\n",
    "    lst_attention_heads = []\n",
    "    for i in range(0,n_h):\n",
    "        WiQ = 'W' + str(i) + 'Q'\n",
    "        WiK = 'W' + str(i) + 'K'\n",
    "        WiV = 'W' + str(i) + 'V'\n",
    "\n",
    "        WQ = W[i][WiQ]\n",
    "        WK = W[i][WiK]\n",
    "        WV = W[i][WiV]\n",
    "        \n",
    "        # get query, key and values\n",
    "        Q, K, V = get_q_k_v(query_data, key_data, value_data, WQ, WK, WV)\n",
    "        \n",
    "        # get n number of head and append to the list\n",
    "        head = calaculate_single_attention(Q, K, V, dk)\n",
    "        lst_attention_heads.append(head)\n",
    "        \n",
    "    # concatinate all heads, get weights for multi head attention multiplication\n",
    "    concat_head = tf.concat(lst_attention_heads, 1) \n",
    "\n",
    "    # perform linear action with weights and concatinated heads above\n",
    "    Z_ = tf.matmul(concat_head, mha_W)\n",
    "    \n",
    "    return Z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will add and normalize \n",
    "def add_norm(data1, data2):\n",
    "    '''\n",
    "    parameters:\n",
    "        data1: input data after some operations\n",
    "        data2: skipped data, bypass from the operations\n",
    "    return:\n",
    "        addNorm: added and normalized values\n",
    "    '''\n",
    "\n",
    "    addNorm = tf.nn.dropout(tf.add(data1, data2), rate = Pdrop)\n",
    "    return addNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function perform position-wise feed forward NN\n",
    "def feed_forward_layer(attention_norm, W1, W2, b1, b2):\n",
    "    '''\n",
    "    Parameters:\n",
    "        attention_norm: input data after attention and normalization\n",
    "        ffn_W: Weights for Feed forward net\n",
    "        b_weights: Bias\n",
    "    return:\n",
    "        z2: activation of the network\n",
    "    '''\n",
    "#     W1 = ffn_W['ffnW1']\n",
    "#     W2 = ffn_W['ffnW2']\n",
    "\n",
    "#     b1 = b_weights['b1']\n",
    "#     b2 = b_weights['b2']\n",
    "\n",
    "    z1 = tf.add(tf.matmul(attention_norm, W1), b1)\n",
    "    a1 = tf.nn.relu(z1)\n",
    "\n",
    "    max_a1 = tf.maximum(0.0, a1)\n",
    "\n",
    "    z2 = tf.add(tf.matmul(max_a1, W2), b2)\n",
    "    \n",
    "    return z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function stack multiple function and works as encoder\n",
    "def encode_layer(S, dk, n_h, W, mha_W, ffn_W, b_weights):\n",
    "    '''\n",
    "    parameters:\n",
    "        S: tensflow PE data\n",
    "        dk: number of query\n",
    "        n_h: number of attention head\n",
    "        W: attention head weights\n",
    "        mha_W: weights to concat multihead\n",
    "        ffn_W: feed forward network weights\n",
    "        b_weights: Bias for feed forward network\n",
    "    return:\n",
    "        encode: encoded value from the encoder layer\n",
    "    '''\n",
    "    W1 = ffn_W['ffnW1']\n",
    "    W2 = ffn_W['ffnW2']\n",
    "\n",
    "    b1 = b_weights['b1']\n",
    "    b2 = b_weights['b2']\n",
    "    \n",
    "    Z_ = multi_head_attention(query_data=S, key_data=S, value_data=S, dk=dk, n_h=n_h, W=W, mha_W=mha_W)\n",
    "    attention_norm = add_norm(Z_, S) # in my computational grapy attention_norm is x\n",
    "#     X_ = feed_forward_layer(attention_norm, ffn_W, b_weights)\n",
    "    X_ = feed_forward_layer(attention_norm, W1, W2, b1, b2)\n",
    "    encode_norm = add_norm(X_, attention_norm) # in my computational grapy encode_norm is encode\n",
    "    \n",
    "    return encode_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add positional encoding to the the data and convert to tensorflow format\n",
    "S = positional_encoding(S, N, steps)\n",
    "S = convert_numpyPE_to_tfPE(S, N, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encode_value = encode_layer(S, dk, n_h, W, mha_W, ffn_W, b_weights)\n",
    "\n",
    "# encode_value = encode_layer(encode_value, dk, n_h, W, mha_W, ffn_W, b_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDECODER STACK\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DECODER STACK\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_Actual = A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = positional_encoding(A, N, steps)\n",
    "A = convert_numpyPE_to_tfPE(A, N, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will evaluate masked multihead attention for the output (shifted right )\n",
    "def masked_multi_head_attention(query_data, key_data, value_data, dk, n_h, md_W, m_mha_W, time_stamp):\n",
    "    '''\n",
    "    parameters:\n",
    "        query_data: data to calculate query\n",
    "        key_data: data to calculate key\n",
    "        value_data: data to calculate value\n",
    "        dk: number of query\n",
    "        n_h: number of heads\n",
    "        md_W: output decoder masked weight  ## for now only\n",
    "        m_mha_W: masked_multihead_attention_weights\n",
    "    return:\n",
    "        Z_: concated single head\n",
    "    '''\n",
    "        \n",
    "    lst_masked_attention_heads = []\n",
    "    for i in range(0,n_h):\n",
    "        WiQ = 'md_W' + str(i) + 'Q'\n",
    "        WiK = 'md_W' + str(i) + 'K'\n",
    "        WiV = 'md_W' + str(i) + 'V'\n",
    "\n",
    "        WQ = md_W[i][WiQ]\n",
    "        WK = md_W[i][WiK]\n",
    "        WV = md_W[i][WiV]\n",
    "    #     print(WQ)\n",
    "\n",
    "        # get n number of head and append to the list\n",
    "#         head = calaculate_single_attention(A, WQ, WK, WV, dk, mask = True)\n",
    "        \n",
    "        # get query, key and values\n",
    "        Q, K, V = get_q_k_v(query_data, key_data, value_data, WQ, WK, WV)\n",
    "        \n",
    "        # get n number of head and append to the list\n",
    "        head = calaculate_single_attention(Q, K, V, dk, mask = True, time_stamp = time_stamp)\n",
    "        \n",
    "        lst_masked_attention_heads.append(head)\n",
    "\n",
    "\n",
    "    # lst_masked_attention_heads    \n",
    "    # concatinate all heads, get weights for multi head attention multiplication\n",
    "    concat_head = tf.concat(lst_masked_attention_heads, 1) \n",
    "\n",
    "    # perform linear action with weights and concatinated heads above\n",
    "    Z_ = tf.matmul(concat_head, m_mha_W)\n",
    "    \n",
    "    return Z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will evaluate masked multihead attention for the output (shifted right )\n",
    "def decoder_multi_head_attention(query_data, key_data, value_data, dk, n_h, d_W, d_mha_W):\n",
    "    '''\n",
    "    parameters:\n",
    "        query_data: data to calculate query\n",
    "        key_data: data to calculate key\n",
    "        value_data: data to calculate value\n",
    "        dk: number of query\n",
    "        n_h: number of heads\n",
    "        d_W: decoder weight  ## for now only\n",
    "        d_mha_W: decoder_multihead_attention_weights\n",
    "    return:\n",
    "        Z_: concated single head\n",
    "    '''\n",
    "    lst_decode_attention_heads = []\n",
    "    for i in range(0,n_h):\n",
    "        WiQ = 'd_W' + str(i) + 'Q'\n",
    "        WiK = 'd_W' + str(i) + 'K'\n",
    "        WiV = 'd_W' + str(i) + 'V'\n",
    "\n",
    "        WQ = d_W[i][WiQ]\n",
    "        WK = d_W[i][WiK]\n",
    "        WV = d_W[i][WiV]\n",
    "\n",
    "        # get query, key and values\n",
    "        Q, K, V = get_q_k_v(query_data, key_data, value_data, WQ, WK, WV)\n",
    "\n",
    "        # get n number of head and append to the list\n",
    "        head = calaculate_single_attention(Q, K, V, dk)\n",
    "\n",
    "        lst_decode_attention_heads.append(head)\n",
    "\n",
    "    concat_head = tf.concat(lst_decode_attention_heads, 1) \n",
    "\n",
    "\n",
    "    # perform linear action with weights and concatinated heads above\n",
    "    Z_ = tf.matmul(concat_head, d_mha_W)\n",
    "    \n",
    "    return Z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function stack multiple function and works as encoder\n",
    "def decoder_layer(A, encode_value, dk, n_h, md_W, m_mha_W, d_W, d_mha_W, d_ffn_W, d_b_weights, time_stamp=0):\n",
    "    '''\n",
    "    parameters:\n",
    "        A: tensflow PE data\n",
    "        encode_value: encoded value from the encoder layer\n",
    "        dk: number of query\n",
    "        n_h: number of attention head\n",
    "        md_W: masked attention weights\n",
    "        d_W: decoder attention head weights\n",
    "        m_mha_W: weights to concat masked multihead \n",
    "        d_mha_W: decoder multihead attention weight\n",
    "        d_ffn_W: decoder feed forward network weights\n",
    "        d_b_weights: decoder Bias for feed forward network\n",
    "    return:\n",
    "        decode: decoded value from the decoder layer\n",
    "    '''\n",
    "\n",
    "    W1 = d_ffn_W['d_ffnW1']\n",
    "    W2 = d_ffn_W['d_ffnW2']\n",
    "\n",
    "    b1 = d_b_weights['d_b1']\n",
    "    b2 = d_b_weights['d_b2']\n",
    "\n",
    "    Z_m = masked_multi_head_attention(query_data=A, key_data=A, value_data=A, dk=dk, n_h=n_h, md_W = md_W,\n",
    "                                      m_mha_W =m_mha_W, time_stamp = time_stamp)\n",
    "    masked_attention_norm = add_norm(Z_m, A) # in my computational grapy masked_attention_norm is x\n",
    "\n",
    "    Z_d = decoder_multi_head_attention(query_data = masked_attention_norm, key_data = encode_value, value_data = encode_value,\n",
    "                                 dk=dk, n_h=n_h, d_W=d_W, d_mha_W=d_mha_W)\n",
    "    decoder_attention_norm = add_norm(Z_d, masked_attention_norm)\n",
    "\n",
    "    X_ = feed_forward_layer(decoder_attention_norm, W1, W2, b1, b2)\n",
    "    decode = add_norm(X_, decoder_attention_norm)\n",
    "    \n",
    "    return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-e302770d9fad>:13: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "decode_value = decoder_layer(A, encode_value, dk, n_h, md_W, m_mha_W, d_W, d_mha_W, d_ffn_W, d_b_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will get prediction from the decoder layer\n",
    "def get_prediction(decode_value, A, W_out, b_out):\n",
    "    '''\n",
    "    parameters:\n",
    "        decode_value: decoded value from the decoder layer\n",
    "        A: output values\n",
    "        W_out: weight for the output layer\n",
    "        b_out: bias for the output layer\n",
    "    return:\n",
    "        predicted_class: prediction from the decoder values\n",
    "        '''\n",
    "    Z = tf.matmul(decode_value,W_out) + b_out\n",
    "    \n",
    "    '''if 2 outputs use sigmoid'''\n",
    "#     output_prob = tf.nn.sigmoid(Z)\n",
    "    \n",
    "    '''if more than 2 outputs use softmax'''\n",
    "    output_prob = tf.nn.softmax(Z)\n",
    "\n",
    "    predicted_indices = tf.argmax(output_prob, dimension=1,name=\"predictions\")\n",
    "\n",
    "    predicted_class = tf.gather(A, predicted_indices)\n",
    "    \n",
    "    return predicted_class, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predicted_class, labels=A))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-40-8a77035b3aec>:20: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    }
   ],
   "source": [
    "predicted_class, _ = get_prediction(decode_value, A, W_out, b_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will convert array values to one hot encoding values\n",
    "def get_one_hot_encoding(A_Actual, n_c):\n",
    "    '''\n",
    "    parameters:\n",
    "        A_Actual: lable data\n",
    "    return:\n",
    "        b: one hot encoded values of labels'''\n",
    "    \n",
    "    a = A_Actual.reshape(A_Actual.shape[0],)\n",
    "    b = np.zeros((a.size, a.max()+1))\n",
    "    b[np.arange(a.size),a] = 1\n",
    "    b\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_Y = A_Actual.astype(np.float32)\n",
    "\n",
    "# losses = tf.nn.sigmoid_cross_entropy_with_logits(labels = output_Y, logits = logits)\n",
    "\n",
    "# cost = tf.reduce_mean(losses)\n",
    "\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=0.007).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "\n",
    "# run the last node of the graph in this code\n",
    "init = tf.global_variables_initializer() \n",
    "\n",
    "# sess = tf.Session()\n",
    "\n",
    "sess = int_sess\n",
    "\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will replace output array with unique numbers\n",
    "def convert_to_unique_numbers(A_Actual):\n",
    "    '''\n",
    "    parameters:\n",
    "        A_Actual: label values\n",
    "    return:\n",
    "        numbers: list of unique number \n",
    "    '''\n",
    "    \n",
    "    names = A_Actual.tolist()\n",
    "    d = {ni: indi for indi, ni in enumerate(set(names))}\n",
    "    numbers = [d[ni] for ni in names]\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rehape output array to 1D array\n",
    "A_Actual = A_Actual.reshape(A_Actual.shape[0],)\n",
    "numbers = convert_to_unique_numbers(A_Actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for binary class'''\n",
    "# output_Y = A_Actual.astype(np.float32)\n",
    "\n",
    "# '''for multiclass get one hot encoded values '''\n",
    "output_Y = get_one_hot_encoding(np.array(numbers), n_c).astype(np.float32)\n",
    "\n",
    "# n_y -- scalar, number of classes    \n",
    "X = tf.placeholder(tf.float32, [steps, N])\n",
    "# Y = tf.placeholder(tf.float32, [None, n_y])\n",
    "Y = tf.placeholder(tf.float32, [output_Y.shape[0], output_Y.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, steps+1):\n",
    "# for i in range(1, 10):\n",
    "    print(i)\n",
    "    A_tsi = sess.run([predicted_class])[0].astype(int) # tsi: time stamp at ith value\n",
    "    \n",
    "    if i == steps: #9:#steps---A.shape[0]\n",
    "        A_hat = A_tsi\n",
    "    \n",
    "    A_tsi = positional_encoding(A_tsi, N, steps)\n",
    "\n",
    "    A_tsi = convert_numpyPE_to_tfPE(A_tsi, N, steps)\n",
    "\n",
    "    decode_value = decoder_layer(A_tsi, encode_value, dk, n_h, md_W, m_mha_W, d_W, d_mha_W, d_ffn_W, d_b_weights, time_stamp=i)\n",
    "    \n",
    "    if i == steps: #9:#steps---A.shape[0]\n",
    "        predicted_class, logits = get_prediction(decode_value, A, W_out, b_out) # logits is fc layer (last)\n",
    "    else:\n",
    "        predicted_class, _ = get_prediction(decode_value, A, W_out, b_out)\n",
    "\n",
    "sess.close()\n",
    "\n",
    "\n",
    "# int_sess.close()\n",
    "\n",
    "#     A_tsi = sess.run([predicted_class])\n",
    "# A_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_3081:0' shape=(512, 5) dtype=float32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = tf.nn.sigmoid_cross_entropy_with_logits(labels = output_Y, logits = logits)\n",
    "\n",
    "cost = tf.reduce_mean(losses)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.007,\n",
    "                                   beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False).minimize(cost)\n",
    "\n",
    "\n",
    "# run the last node of the graph in this code\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)  \n",
    "    \n",
    "    cost_lst = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        _ , epoch_cost = sess.run([optimizer, cost], feed_dict={X: input_X, Y: output_Y})\n",
    "        \n",
    "        cost_lst.append(epoch_cost)\n",
    "        \n",
    "        print('Epoch: ', epoch, ' Cost : ', epoch_cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.ylim(top=1000)\n",
    "# print('Transformer on Sample Data')\n",
    "plt.plot(np.log(cost_lst))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error on Logramathic scale')\n",
    "plt.title('Transformer on Games Data \\n Steps = '+ str(steps))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.axes.set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
